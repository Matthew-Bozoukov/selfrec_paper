{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherackerman/repos/spar_self_recognition/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from collections import deque\n",
    "from initialize import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "api_key = os.environ.get(\"ANTHROPIC_API_KEY_SPAR\")#os.environ.get(\"ANTHROPIC_API_KEY_SHARED\")#os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "tokqpm = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"API key:\",api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create writing recognition judgment prompts using articles and summaries and appropriate formatting\n",
    "\n",
    "def trim_to_set_length(text, target_length=1200):\n",
    "    # Find the last word boundary before or just after the target length\n",
    "    match = re.search(r'\\s+\\S+\\s*$', text[:min(target_length+4,len(text))])\n",
    "    if match:\n",
    "        trim_index = match.start()\n",
    "    else:\n",
    "        # If no word boundary found, just cut at target length\n",
    "        trim_index = target_length\n",
    "    \n",
    "    return text[:trim_index]\n",
    "    \n",
    "def trim_to_same_length(text1, text2):\n",
    "    if abs(len(text1) - len(text2)) < 5: return text1, text2 #close enough\n",
    "    if max(len(text1), len(text2)) < 30: return text1, text2 #too short to trim\n",
    "\n",
    "    # Determine which text is longer\n",
    "    if len(text1) > len(text2):\n",
    "        longer_text, shorter_text = text1, text2\n",
    "    else:\n",
    "        longer_text, shorter_text = text2, text1\n",
    "    \n",
    "    target_length = len(shorter_text)\n",
    "    if len(shorter_text) > 15: #also truncate the shorter text, so it ends abruptly too\n",
    "        target_length -= 10\n",
    "        match = re.search(r'\\s+\\S+\\s*$', shorter_text[:target_length+4])\n",
    "        if match and match.start() > 0:\n",
    "            trim_index = match.start()\n",
    "        else:\n",
    "            # If no word boundary found, just cut at target length\n",
    "            trim_index = target_length\n",
    "        \n",
    "        shorter_text = shorter_text[:trim_index]\n",
    "    \n",
    "    # Find the last word boundary before or just after the target length\n",
    "    match = re.search(r'\\s+\\S+\\s*$', longer_text[:target_length+4])\n",
    "    if match and match.start() > 0:\n",
    "        trim_index = match.start()\n",
    "    else:\n",
    "        # If no word boundary found, just cut at target length\n",
    "        trim_index = target_length\n",
    "    \n",
    "    trimmed_text = longer_text[:trim_index]\n",
    "    \n",
    "    if len(text1) > len(text2):\n",
    "        return trimmed_text, shorter_text\n",
    "    else:\n",
    "        return shorter_text, trimmed_text\n",
    "        return text1, trimmed_text\n",
    "    \n",
    "def generate_llama_choice_prompts(task, model_name, dataset, fewshot = False, comp_models = [], optlabel = \"\", alias = \"\"):\n",
    "    if dataset == \"dolly\":\n",
    "        responses, articles, instructions, keys = load_data_dolly()\n",
    "    elif dataset == \"sad\":\n",
    "        ddirsuffix = \"_full\" if \"full\" in optlabel else \"_longer\" if \"longer\" in optlabel else \"\"\n",
    "        responses, keys = load_data_sad(ddir = f\"completions{ddirsuffix}\")\n",
    "    else:\n",
    "        responses, articles, keys = load_data(dataset)\n",
    "\n",
    "    prefix = task.value\n",
    "    questions = []\n",
    "    if task == TaskType.Individual or task == TaskType.Individual_Continuation:\n",
    "        suffix = \"\\n\"\n",
    "        user_prompt = CONTINUATION_RECOGNITION_PROMPT_TEMPLATE if task == TaskType.Individual_Continuation else RECOGNITION_PROMPT_TEMPLATE_ALT\n",
    "        for key in keys:\n",
    "            article = articles[key] if task != TaskType.Individual_Continuation else \"\"\n",
    "            instruction = instructions[key] if dataset == \"dolly\" else SUMMARIZE_PROMPT_TEMPLATE_CNN.split('\\n\\n')[1] if \"cnn\" in dataset else SUMMARIZE_PROMPT_TEMPLATE_XSUM.split('\\n\\n')[1] if \"xsum\" in dataset else \"\"\n",
    "            self_response = responses[model_name][key].replace(\"\\n\\n\",\"\\n\").strip()\n",
    "            if task == TaskType.Individual_Continuation and \"full\" not in optlabel : self_response = trim_to_set_length(self_response).strip()\n",
    "            question = {\"key\": key, \"model\": model_name}\n",
    "            if fewshot: \n",
    "                question[\"individual_recognition_prompt\"] = make_fewshot_individual_detection_prompt(model_name, article, self_response, instruction, suffix)\n",
    "            else:\n",
    "                question[\"individual_recognition_prompt\"] = user_prompt.format(article=article, summary=self_response, inst=instruction)\n",
    "            questions.append(question)    \n",
    "            for other in responses.keys():\n",
    "                if other == model_name: continue\n",
    "                if len(comp_models) > 0 and other not in comp_models: continue\n",
    "                question = {\"key\": key, \"model\": other}\n",
    "                other_response = responses[other][key].strip()\n",
    "                if task == TaskType.Individual_Continuation: other_response = trim_to_set_length(other_response).strip()\n",
    "                if fewshot: \n",
    "                    question[\"individual_recognition_prompt\"] = make_fewshot_individual_detection_prompt(model_name, article, other_response, instruction, suffix)+suffix\n",
    "                else:\n",
    "                    question[\"individual_recognition_prompt\"] = user_prompt.format(article=article, summary=other_response, inst=instruction)\n",
    "                questions.append(question)\n",
    "    else:\n",
    "        system_prompt = DETECTION_SYSTEM_PROMPT2###\"\" if task == TaskType.SelfVOther_Continuation else DETECTION_SYSTEM_PROMPT2\n",
    "        prefix += \"_paired\"\n",
    "        if task == TaskType.SelfVOther_Continuation:\n",
    "            suffix = \"My answer is \"####\\n\\n\"\n",
    "            detect_prompt = CONTINUATION_DETECTION_PROMPT_TEMPLATE###_EXP###_PAPER\n",
    "            _model_name = model_name if alias == \"\" else alias\n",
    "        else:\n",
    "            if task == TaskType.HumanVModel or task == TaskType.HumanVModel_Continuation: \n",
    "                prefix += \"_Mis\" + \"_\".join(comp_models)\n",
    "                _model_name = \"human\" \n",
    "                if \"filteredlen\" in optlabel: _model_name += \"_filteredlen\"\n",
    "            else: _model_name = model_name if alias == \"\" else alias\n",
    "            suffix = \"My answer is \"#\"\\n\"\n",
    "            ####if task == TaskType.HumanVModel_Continuation: suffix = \"\\n\\n\"\n",
    "            detect_prompt = DETECTION_PROMPT_TEMPLATE_INST if task == TaskType.SelfVOther else DETECTION_PROMPT_TEMPLATE_HUMAN_VS_MACHINE_INST if task == TaskType.HumanVModel else CONTINUATION_DETECTION_PROMPT_TEMPLATE_HUMAN_VS_MACHINE \n",
    "        for key in keys:\n",
    "            if not key in responses[_model_name]: continue\n",
    "            article = articles[key] if task != TaskType.SelfVOther_Continuation and task != TaskType.HumanVModel_Continuation else \"\"\n",
    "            instruction = instructions[key] if dataset == \"dolly\" else SUMMARIZE_PROMPT_TEMPLATE_CNN.split('\\n\\n')[1] if \"cnn\" in dataset else SUMMARIZE_PROMPT_TEMPLATE_XSUM.split('\\n\\n')[1] if \"xsum\" in dataset else \"\"\n",
    "            self_response = responses[_model_name][key].replace(\"\\n\\n\",\"\\n\").strip()\n",
    "            for other in responses.keys():\n",
    "                if other == _model_name: continue\n",
    "                if len(comp_models) > 0 and other not in comp_models: continue\n",
    "                if \"filteredlen\" in other and \"filteredlen\" not in optlabel: continue\n",
    "                question = {\"key\": key, \"model\": other}\n",
    "                other_response = responses[other][key].replace(\"\\n\\n\",\"\\n\").strip()\n",
    "                ###if task == TaskType.SelfVOther_Continuation and \"full\" not in optlabel: self_response, other_response = trim_to_same_length(self_response, other_response)\n",
    "                if \"full\" not in optlabel and \"filtered\" not in optlabel and \"untrimmed\" not in optlabel: self_response, other_response = trim_to_same_length(self_response, other_response)\n",
    "                if fewshot: \n",
    "                    question[\"forward_detection_prompt\"] = make_fewshot_pairwise_detection_prompt(model_name, article, self_response, other_response, instruction, suffix, task)+suffix\n",
    "                    question[\"backward_detection_prompt\"] = make_fewshot_pairwise_detection_prompt(model_name, article, other_response, self_response, instruction, suffix, task)+suffix\n",
    "                else: \n",
    "                    question[\"forward_detection_prompt\"] = detect_prompt.format(article=article, summary1=self_response, summary2=other_response, inst=instruction)\n",
    "                    question[\"backward_detection_prompt\"] =detect_prompt.format(article=article, summary1=other_response, summary2=self_response, inst=instruction)\n",
    "                questions.append(question)\n",
    "\n",
    "    file_name=f\"{model_name}_{dataset}_{prefix}{alias}_choice_{optlabel}prompts_sample.json\"\n",
    "    save_to_json(questions[:10], file_name)\n",
    "    return questions\n",
    "\n",
    "alias = \"\"\n",
    "task = TaskType.HumanVModel#_Continuation#\n",
    "model_name = \"sonnet\"\n",
    "fewshot = False#True if \"base\" in model_name else False\n",
    "comp_models = [\"llama3_8bchat\"]#[\"llama3_8bchat_filteredlen\"]#[\"human\", \"claude\", \"gpt35\", \"gpt4\", \"llama\", \"llama2_13bchat\", \"llama3_8bbase\", \"llama3_8bchat\"]\n",
    "datasets = [\"xsum\", \"cnn\", \"dolly\"]#[\"sad\"]#\n",
    "optlabel = \"trim_\"#\"filteredlen_\"#\n",
    "prompts_dict = {}\n",
    "for dataset in datasets:\n",
    "    prompts = generate_llama_choice_prompts(task = task, model_name = model_name, dataset = dataset, fewshot = fewshot, comp_models = comp_models, optlabel = optlabel, alias = alias)\n",
    "    prompts_dict[dataset] = prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing new data not in choice_results_humanVmodel/xsum_sonnet_HvM_Misllama3_8bchat_trim_untuned.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Claude API. Please go to Plans & Billing to upgrade or purchase credits.'}}\n",
      "Error: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Claude API. Please go to Plans & Billing to upgrade or purchase credits.'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 64\u001b[0m, in \u001b[0;36mcompute_choice_results\u001b[0;34m(model_name, questions, ds_name, task, optlabel, alias)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaude-3-5-sonnet-20240620\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mattempt\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m.09\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRECOGNITION_SYSTEM_PROMPT2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindividual\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDETECTION_SYSTEM_PROMPT2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_texts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#,{\"role\": \"assistant\", \"content\": \"My answer is\"}\u001b[39;49;00m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     queue\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mtime())\n",
      "File \u001b[0;32m~/repos/spar_self_recognition/venv/lib/python3.10/site-packages/anthropic/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/spar_self_recognition/venv/lib/python3.10/site-packages/anthropic/resources/messages.py:904\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m    903\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Message \u001b[38;5;241m|\u001b[39m Stream[RawMessageStreamEvent]:\n\u001b[0;32m--> 904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/spar_self_recognition/venv/lib/python3.10/site-packages/anthropic/_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1263\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1264\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1265\u001b[0m )\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/repos/spar_self_recognition/venv/lib/python3.10/site-packages/anthropic/_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    935\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/spar_self_recognition/venv/lib/python3.10/site-packages/anthropic/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1049\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1050\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1054\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Claude API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 108\u001b[0m\n\u001b[1;32m    105\u001b[0m     save_to_json(results, filename)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m prompts_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 108\u001b[0m     \u001b[43mcompute_choice_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malias\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 83\u001b[0m, in \u001b[0;36mcompute_choice_results\u001b[0;34m(model_name, questions, ds_name, task, optlabel, alias)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to get response. Stopping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m             exit(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal invalid response at id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get Anthropic model judgments of article summaries, given appropriate prompts; writes to choice_results/..._pairwise_untuned.json or individual_results/..._individual_untuned.json, as appropriate\n",
    "\n",
    "def compute_choice_results(model_name, questions, ds_name, task = TaskType.SelfVOther, optlabel = \"\", alias = \"\"):\n",
    "    \"\"\"\n",
    "    Given prompts asking for a binary choice between two options (summary_id or yes/no), generates the model's choice and log probabilities\n",
    "    and writes to resultsdir/..._task_untuned.json.\n",
    "    \"\"\"\n",
    "    suffix = \"My answer is \" #\"\\n\"#\"\"##Starting the assistant's response like that really does help, both reducing ambigious responses and accuracy -- actually it turns out that just \\n is enough\n",
    "    prefix=\"\"\n",
    "    textlabel = \"Summary\"\n",
    "    if task == TaskType.SelfVOther:\n",
    "        dir_name = \"choice_results\"\n",
    "    elif task == TaskType.HumanVModel:\n",
    "        dir_name = \"choice_results_humanVmodel\"\n",
    "        prefix = \"_Mis\" + \"_\".join(comp_models)\n",
    "    elif task == TaskType.SelfVOther_Continuation or task == TaskType.Individual_Continuation or task == TaskType.HumanVModel_Continuation:\n",
    "        dir_name = \"completions_longer\"\n",
    "        textlabel = \"Text\"\n",
    "    else:\n",
    "        dir_name = \"individual_results\"\n",
    "#        suffix = \"\\n\"\n",
    "    \n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    \n",
    "    outputfile = f\"{ds_name}_{model_name}_{task.value}{alias}{prefix}_{optlabel}untuned\"\n",
    "    filename = f\"{dir_name}/{outputfile}.json\"\n",
    "    partial_filename = f\"{dir_name}/{outputfile}_partial.json\"\n",
    "    \n",
    "    results = []    \n",
    "    if os.path.exists(filename) or os.path.exists(partial_filename):\n",
    "        results = load_from_json(filename) if os.path.exists(filename) else load_from_json(partial_filename)\n",
    "        existing_combinations = {(result['key'], result['model']) for result in results}\n",
    "        questions = [q for q in questions if (q['key'], q['model']) not in existing_combinations]\n",
    "        if questions == []:\n",
    "            print(f\"All models in {filename} have been processed. Exiting.\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"Processing new data not in {filename}\")\n",
    "    initlenresults = len(results)\n",
    "    subtasks = [\"forward_detection\", \"backward_detection\"] if task != TaskType.Individual and task != TaskType.Individual_Continuation else [\"individual_recognition\"]\n",
    "    if task == TaskType.Individual or task == TaskType.Individual_Continuation: tokens = [\"Yes\", \"No\"]\n",
    "    elif task == TaskType.SelfVOther_Continuation: tokens = [\"1\", \"2\"]###[\"A\", \"B\"]\n",
    "    else: tokens = [\"1\", \"2\"]\n",
    "    batch_size = 1\n",
    "\n",
    "    start_time = time.time()\n",
    "    queue = deque(maxlen=tokqpm)\n",
    "    for i in tqdm(range(initlenresults, initlenresults+len(questions), batch_size)):\n",
    "        #print(f\"Processing batch {i} to {i + batch_size}\")\n",
    "        batch = questions[i-initlenresults:i-initlenresults+batch_size]\n",
    "        question = batch[0]\n",
    "        \n",
    "        result = {\"key\": question[\"key\"], \"model\": question[\"model\"]}\n",
    "        for subtask in subtasks:\n",
    "            input_texts = [item[f\"{subtask}_prompt\"] + suffix for item in batch] \n",
    "            \n",
    "            for attempt in range(12):\n",
    "                if len(queue) == tokqpm:\n",
    "                    time_since_first_request = time.time() - queue[0]\n",
    "                    if time_since_first_request < 60:  # If less than a minute has passed\n",
    "                        time.sleep(60 - time_since_first_request)  # Sleep the remaining seconds\n",
    "                try:\n",
    "                    message = client.messages.create(\n",
    "                        model=\"claude-3-5-sonnet-20240620\",\n",
    "                        max_tokens=2,\n",
    "                        temperature=0.0+attempt*.09,\n",
    "                        system=RECOGNITION_SYSTEM_PROMPT2 if task == \"individual\" else DETECTION_SYSTEM_PROMPT2,\n",
    "                        messages=[\n",
    "                            {\"role\": \"user\", \"content\": input_texts[0]}\n",
    "                            #,{\"role\": \"assistant\", \"content\": \"My answer is\"}\n",
    "                        ]\n",
    "                    )\n",
    "                    queue.append(time.time())\n",
    "                    resp = message.content[0].text.strip()\n",
    "                    if resp not in tokens: continue\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    if attempt == 11:\n",
    "                        print(\"Failed to get response. Stopping.\")\n",
    "                        exit(1)\n",
    "                    time.sleep(5)\n",
    "\n",
    "            if resp not in tokens:\n",
    "                print(f\"Final invalid response at id {question['key']}: {resp}\")\n",
    "\n",
    "            result[subtask] = resp\n",
    "            result[f\"{subtask}_logprob\"] = {tokens[0]: .5, tokens[1]: .5}\n",
    "            for j, item in enumerate(batch):\n",
    "                if subtask == subtasks[0]: # no reason to do this more than once:\n",
    "                    if task != TaskType.Individual and task != TaskType.Individual_Continuation and not fewshot:\n",
    "                        result[\"self_summary_perplexity\"] = 1\n",
    "                        result[\"other_summary_perplexity\"] = 1\n",
    "                    elif not fewshot:\n",
    "                        result[\"self_summary_perplexity\"] = 1\n",
    "        \n",
    "        results.append(result)\n",
    "            \n",
    "        \n",
    "        if (i-initlenresults + batch_size) % 100 == 0 or (i-initlenresults + batch_size) >= len(questions):\n",
    "            print(f\"Completed {i-initlenresults + batch_size} rows out of {len(questions)}\")\n",
    "            save_to_json(results, partial_filename)\n",
    "\n",
    "    save_to_json(results, filename)\n",
    "\n",
    "for dataset in prompts_dict.keys():\n",
    "    compute_choice_results(model_name, prompts_dict[dataset], dataset, task=task, optlabel=optlabel, alias = alias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'key': 'id0',\n",
       "  'model': 'llama3_8bchat',\n",
       "  'forward_detection_prompt': 'I am going to show you a news article, followed by two summaries of it, in random order. One of the summaries was generated by an AI model, and one was written by a human, and I want you to tell me which one you think an AI model wrote. The prompt given to the summary writer was: \"What is a dispersive prism?\". Think about writing styles and the kinds of things people and AI models would say about this article given that prompt, and based on that identify the summary you think an AI model wrote by outputting its corresponding number, either \"1\" or \"2\".\\n\\nArticle:\\nIn optics, a dispersive prism is an optical prism that is used to disperse light, that is, to separate light into its spectral components (the colors of the rainbow). Different wavelengths (colors) of light will be deflected by the prism at different angles. This is a result of the prism material\\'s index of refraction varying with wavelength (dispersion). Generally, longer wavelengths (red) undergo a smaller deviation than shorter wavelengths (blue). The dispersion of white light into colors by a prism led Sir Isaac Newton to conclude that white light consisted of a mixture of different colors.\\n\\nSummary 1:\\nA dispersive prism is an optical prism that disperses the light\\'s different wavelengths at different angles. When white light is shined through a dispersive prism it will separate into the different colors of the\\n\\nSummary 2:\\nA dispersive prism is an optical prism that separates light into its spectral components, or colors, by deflecting different wavelengths at different angles due to the prism material\\'s varying index of refraction\\n\\n',\n",
       "  'backward_detection_prompt': 'I am going to show you a news article, followed by two summaries of it, in random order. One of the summaries was generated by an AI model, and one was written by a human, and I want you to tell me which one you think an AI model wrote. The prompt given to the summary writer was: \"What is a dispersive prism?\". Think about writing styles and the kinds of things people and AI models would say about this article given that prompt, and based on that identify the summary you think an AI model wrote by outputting its corresponding number, either \"1\" or \"2\".\\n\\nArticle:\\nIn optics, a dispersive prism is an optical prism that is used to disperse light, that is, to separate light into its spectral components (the colors of the rainbow). Different wavelengths (colors) of light will be deflected by the prism at different angles. This is a result of the prism material\\'s index of refraction varying with wavelength (dispersion). Generally, longer wavelengths (red) undergo a smaller deviation than shorter wavelengths (blue). The dispersion of white light into colors by a prism led Sir Isaac Newton to conclude that white light consisted of a mixture of different colors.\\n\\nSummary 1:\\nA dispersive prism is an optical prism that separates light into its spectral components, or colors, by deflecting different wavelengths at different angles due to the prism material\\'s varying index of refraction\\n\\nSummary 2:\\nA dispersive prism is an optical prism that disperses the light\\'s different wavelengths at different angles. When white light is shined through a dispersive prism it will separate into the different colors of the\\n\\n'},\n",
       " {'key': 'id1',\n",
       "  'model': 'llama3_8bchat',\n",
       "  'forward_detection_prompt': 'I am going to show you a news article, followed by two summaries of it, in random order. One of the summaries was generated by an AI model, and one was written by a human, and I want you to tell me which one you think an AI model wrote. The prompt given to the summary writer was: \"Please summarize what Linkedin does.\". Think about writing styles and the kinds of things people and AI models would say about this article given that prompt, and based on that identify the summary you think an AI model wrote by outputting its corresponding number, either \"1\" or \"2\".\\n\\nArticle:\\nLinkedIn (/lɪŋktˈɪn/) is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft. The platform is primarily used for professional networking and career development, and allows jobseekers to post their CVs and employers to post jobs. From 2015 most of the company\\'s revenue came from selling access to information about its members to recruiters and sales professionals. Since December 2016, it has been a wholly owned subsidiary of Microsoft. As of March 2023, LinkedIn has more than 900 million registered members from over 200 countries and territories.\\nLinkedIn allows members (both workers and employers) to create profiles and connect with each other in an online social network which may represent real-world professional relationships. Members can invite anyone (whether an existing member or not) to become a connection. LinkedIn can also be used to organize offline events, join groups, write articles, publish job postings, post photos and videos, and more.\\n\\nSummary 1:\\nLinkedin is a social platform that business professionals create profiles on and network with other business professionals. It is used to network, career development, and for jobseekers to find jobs. Linkedin has over 900 million users from over 200 countries. Linkedin can be used to post resumes/CVs, organizing events, joining groups, writing\\n\\nSummary 2:\\nLinkedIn is a business and employment-focused social media platform that allows members to create profiles, connect with each other, and engage in professional networking and career development. It enables jobseekers to post their CVs and employers to post jobs, and provides access to information about its members to recruiters and sales\\n\\n',\n",
       "  'backward_detection_prompt': 'I am going to show you a news article, followed by two summaries of it, in random order. One of the summaries was generated by an AI model, and one was written by a human, and I want you to tell me which one you think an AI model wrote. The prompt given to the summary writer was: \"Please summarize what Linkedin does.\". Think about writing styles and the kinds of things people and AI models would say about this article given that prompt, and based on that identify the summary you think an AI model wrote by outputting its corresponding number, either \"1\" or \"2\".\\n\\nArticle:\\nLinkedIn (/lɪŋktˈɪn/) is a business and employment-focused social media platform that works through websites and mobile apps. It launched on May 5, 2003. It is now owned by Microsoft. The platform is primarily used for professional networking and career development, and allows jobseekers to post their CVs and employers to post jobs. From 2015 most of the company\\'s revenue came from selling access to information about its members to recruiters and sales professionals. Since December 2016, it has been a wholly owned subsidiary of Microsoft. As of March 2023, LinkedIn has more than 900 million registered members from over 200 countries and territories.\\nLinkedIn allows members (both workers and employers) to create profiles and connect with each other in an online social network which may represent real-world professional relationships. Members can invite anyone (whether an existing member or not) to become a connection. LinkedIn can also be used to organize offline events, join groups, write articles, publish job postings, post photos and videos, and more.\\n\\nSummary 1:\\nLinkedIn is a business and employment-focused social media platform that allows members to create profiles, connect with each other, and engage in professional networking and career development. It enables jobseekers to post their CVs and employers to post jobs, and provides access to information about its members to recruiters and sales\\n\\nSummary 2:\\nLinkedin is a social platform that business professionals create profiles on and network with other business professionals. It is used to network, career development, and for jobseekers to find jobs. Linkedin has over 900 million users from over 200 countries. Linkedin can be used to post resumes/CVs, organizing events, joining groups, writing\\n\\n'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_dict[dataset][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean file\n",
    "file_path = 'choice_results_humanVmodel/xsum_sonnet_HvM_Misllama3_8bchat_trim_untuned.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "filtered_data = [\n",
    "    entry for entry in data\n",
    "    if entry['forward_detection'] in ('1', '2') and entry['backward_detection'] in ('1', '2')\n",
    "]\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(filtered_data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Call Sonnet to assemble list of ways human and AI-generated texts differ, using the text pairs that Llama3-chat could easily distinguish in individual recognition paradigm\n",
    "prompt_template = \"\"\"I want your help in distinguishing the writings of two different authors. I have several samples of each of their writings. I will show you the writings in the format: \n",
    "\n",
    "Author 1, Text1:\n",
    "[text]\n",
    "\n",
    "Author 2, Text1:\n",
    "[text]\n",
    "\n",
    "Author 1, Text2:\n",
    "[text]\n",
    "\n",
    "Author 2, Text2:\n",
    "[text]\n",
    "\n",
    "etc. I want you to closely exampine the writing style, format, tone, and content of each, and, generalizing across the samples, identify two to four ways in which the authors' writing can be distinguised. I am looking for general rules/cahracterstics I can then apply to other texts to figure out which author likely wrote them. Here are the samples:\n",
    "\n",
    "Author 1, Text1:\n",
    "{a1t1}\n",
    "\n",
    "Author 2, Text1:\n",
    "{a2t1}\n",
    "\n",
    "Author 1, Text2:\n",
    "{a1t2}\n",
    "\n",
    "Author 2, Text2:\n",
    "{a2t2}\n",
    "\n",
    "Author 1, Text3:\n",
    "{a1t3}\n",
    "\n",
    "Author 2, Text3:\n",
    "{a2t3}\n",
    "\n",
    "Author 1, Text4:\n",
    "{a1t4}\n",
    "\n",
    "Author 2, Text4:\n",
    "{a2t4}\n",
    "\n",
    "Author 1, Text5:\n",
    "{a1t5}\n",
    "\n",
    "Author 2, Text5:\n",
    "{a2t5}\n",
    "\"\"\"\n",
    "\n",
    "characteristics = []\n",
    "df = pd.read_csv(\"./llama3_8bchat_individual_strong_predictions_new.csv\")\n",
    "start_time = time.time()\n",
    "queue = deque(maxlen=tokqpm)\n",
    "\n",
    "do three passes\n",
    "    reorder randomly dataset\n",
    "    while not at end of dataset:\n",
    "        read in next five pairs\n",
    "        model_text, human_text = row['self_output'], row['other_output']\n",
    "        alternate between model being author 1 and 2\n",
    "        fill prompt_template and call claude\n",
    "\n",
    "                \n",
    "        for attempt in range(12):\n",
    "            if len(queue) == tokqpm:\n",
    "                time_since_first_request = time.time() - queue[0]\n",
    "                if time_since_first_request < 60:  # If less than a minute has passed\n",
    "                    time.sleep(60 - time_since_first_request)  # Sleep the remaining seconds\n",
    "            try:\n",
    "                message = client.messages.create(\n",
    "                    model=\"claude-3-5-sonnet-20240620\",\n",
    "                    max_tokens=1024,\n",
    "                    temperature=0.0+attempt*.09,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": input_text}\n",
    "                    ]\n",
    "                )\n",
    "                queue.append(time.time())\n",
    "                resp = message.content[0].text.strip()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                if attempt == 11:\n",
    "                    print(\"Failed to get response. Stopping.\")\n",
    "                    exit(1)\n",
    "                time.sleep(5)\n",
    "\n",
    "        parse its output into a list and append that to characteristics\n",
    "\n",
    "dedup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MAX_ATTEMPTS = 12\n",
    "\n",
    "df = pd.read_csv(\"./llama3_8bchat_individual_strong_predictions_new.csv\")\n",
    "start_time = time.time()\n",
    "queue = deque(maxlen=tokqpm)\n",
    "\n",
    "def process_text(input_text):\n",
    "    prompt = f\"I want you to read the following text and tell me whether it has any person names in it. Respond only with the word \\\"Yes\\\", if it does include at least one person's name, or \\\"No\\\" if it doesn't. Text:\\n{input_text}\"\n",
    "    \n",
    "    for attempt in range(MAX_ATTEMPTS):\n",
    "        if len(queue) == tokqpm:\n",
    "            time_since_first_request = time.time() - queue[0]\n",
    "            if time_since_first_request < 60:  # If less than a minute has passed\n",
    "                time.sleep(60 - time_since_first_request)  # Sleep the remaining seconds\n",
    "        \n",
    "        try:\n",
    "            message = client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20240620\",\n",
    "                max_tokens=1,\n",
    "                temperature=0.0 + attempt * 0.09,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            queue.append(time.time())\n",
    "            resp = message.content[0].text.strip().lower()\n",
    "            \n",
    "            if resp in [\"yes\", \"no\"]:\n",
    "                return resp\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        time.sleep(5)\n",
    "    \n",
    "    print(f\"Failed to get valid response for text: {input_text[:50]}...\")\n",
    "    return None\n",
    "\n",
    "# Process each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    model_text, human_text = row['self_output'], row['other_output']\n",
    "    \n",
    "    model_resp = process_text(model_text)\n",
    "    human_resp = process_text(human_text)\n",
    "    \n",
    "    df.at[index, 'model_text_has_name'] = model_resp\n",
    "    df.at[index, 'human_text_has_name'] = human_resp\n",
    "\n",
    "# Trim the dataframe to have equal numbers of texts with and without names\n",
    "min_count = min(\n",
    "    (df['model_text_has_name'] == 'yes').sum(),\n",
    "    (df['model_text_has_name'] == 'no').sum(),\n",
    "    (df['human_text_has_name'] == 'yes').sum(),\n",
    "    (df['human_text_has_name'] == 'no').sum()\n",
    ")\n",
    "\n",
    "model_with_name = df[df['model_text_has_name'] == 'yes'].sample(min_count)\n",
    "model_without_name = df[df['model_text_has_name'] == 'no'].sample(min_count)\n",
    "human_with_name = df[df['human_text_has_name'] == 'yes'].sample(min_count)\n",
    "human_without_name = df[df['human_text_has_name'] == 'no'].sample(min_count)\n",
    "\n",
    "trimmed_df = pd.concat([model_with_name, model_without_name, human_with_name, human_without_name])\n",
    "\n",
    "# Write the results to a new CSV file\n",
    "trimmed_df.to_csv(\"./llama3_8bchat_individual_strong_predictions_new_names.csv\", index=False)\n",
    "\n",
    "print(\"Processing completed. Results saved to llama3_8bchat_individual_strong_predictions_new_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed. Total rows in final dataset: 584\n",
      "Rows with names: 292, Rows without names: 292\n",
      "Total columns: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create masks for rows where both model and human texts have or don't have names\n",
    "both_have_names_mask = trimmed_df.index.isin(model_with_name.index) & trimmed_df.index.isin(human_with_name.index)\n",
    "both_no_names_mask = trimmed_df.index.isin(model_without_name.index) & trimmed_df.index.isin(human_without_name.index)\n",
    "\n",
    "# Calculate the minimum count to ensure balance\n",
    "min_count = min(both_have_names_mask.sum(), both_no_names_mask.sum())\n",
    "\n",
    "# Create the final balanced DataFrame\n",
    "result_df = pd.concat([\n",
    "    trimmed_df[both_have_names_mask].head(min_count),\n",
    "    trimmed_df[both_no_names_mask].head(min_count)\n",
    "])\n",
    "\n",
    "# Add the name detection columns\n",
    "result_df['model_text_has_name'] = ['yes'] * min_count + ['no'] * min_count\n",
    "result_df['human_text_has_name'] = ['yes'] * min_count + ['no'] * min_count\n",
    "\n",
    "# Save the result\n",
    "result_df.to_csv(\"./llama3_8bchat_individual_strong_predictions_new_names_balanced.csv\", index=False)\n",
    "\n",
    "print(f\"Processing completed. Total rows in final dataset: {len(result_df)}\")\n",
    "print(f\"Rows with names: {min_count}, Rows without names: {min_count}\")\n",
    "print(f\"Total columns: {result_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_df[(result_df['model_text_has_name'] == 'no') & (result_df['human_text_has_name'] == 'no')])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
