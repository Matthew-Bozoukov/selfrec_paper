{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Analysis Results:\n",
      "Minimum length: 112\n",
      "Maximum length: 290\n",
      "Median length: 152.00\n",
      "Mean length: 161.86\n",
      "Mean newlines per value: 0.00\n",
      "\n",
      "Top 5 first characters:\n",
      "  'A': 18.8%\n",
      "  'T': 15.5%\n",
      "  'F': 7.2%\n",
      "  'S': 5.8%\n",
      "  'M': 5.8%\n",
      "\n",
      "Top 5 last characters:\n",
      "  '.': 100.0%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from statistics import mean, median\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def analyze_json_strings(json_file_path):\n",
    "    # Read JSON file\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract values and calculate lengths\n",
    "    values = list(data.values())\n",
    "    lengths = [len(v.strip()) for v in values]\n",
    "    #lengths = [len(v.strip()) for v in values if len(v.strip()) > 50]\n",
    "\n",
    "    # Calculate statistics\n",
    "    min_length = min(lengths)\n",
    "    max_length = max(lengths)\n",
    "    median_length = median(lengths)\n",
    "    mean_length = mean(lengths)\n",
    "    \n",
    "    # Calculate mean number of newlines\n",
    "    newlines = [v.count('\\n') for v in (v.strip() for v in values)]\n",
    "    mean_newlines = mean(newlines)\n",
    "    \n",
    "    # Find most common first and last characters\n",
    "    first_chars = [v.strip()[0] for v in values if v]\n",
    "    last_chars = [v.strip()[-1] for v in values if v]\n",
    "    \n",
    "    first_char_counts = Counter(first_chars).most_common(5)\n",
    "    last_char_counts = Counter(last_chars).most_common(5)\n",
    "    \n",
    "    total_values = len(values)\n",
    "    first_char_counts = Counter(first_chars).most_common(5)\n",
    "    last_char_counts = Counter(last_chars).most_common(5)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"String Analysis Results:\")\n",
    "    print(f\"Minimum length: {min_length}\")\n",
    "    print(f\"Maximum length: {max_length}\")\n",
    "    print(f\"Median length: {median_length:.2f}\")\n",
    "    print(f\"Mean length: {mean_length:.2f}\")\n",
    "    print(f\"Mean newlines per value: {mean_newlines:.2f}\")\n",
    "    \n",
    "    print(\"\\nTop 5 first characters:\")\n",
    "    for char, count in first_char_counts:\n",
    "        percentage = (count / total_values) * 100\n",
    "        print(f\"  '{char}': {percentage:.1f}%\")\n",
    "    \n",
    "    print(\"\\nTop 5 last characters:\")\n",
    "    for char, count in last_char_counts:\n",
    "        percentage = (count / total_values) * 100\n",
    "        print(f\"  '{char}': {percentage:.1f}%\")\n",
    "\n",
    "file_path = '/Users/christopherackerman/repos/spar_self_recognition/summaries/xsum_train_human_filteredlen_responses.json'\n",
    "analyze_json_strings(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Analysis Results:\n",
      "Minimum length: 130\n",
      "Maximum length: 231\n",
      "Median length: 171.00\n",
      "Mean length: 173.99\n",
      "Mean newlines per value: 0.00\n",
      "\n",
      "Top 5 first characters:\n",
      "  'A': 19.3%\n",
      "  'T': 16.4%\n",
      "  'S': 8.7%\n",
      "  'B': 6.8%\n",
      "  'C': 5.8%\n",
      "\n",
      "Top 5 last characters:\n",
      "  '.': 100.0%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from statistics import mean, median\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def analyze_json_strings(json_file_path):\n",
    "    # Read JSON file\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract values and calculate lengths\n",
    "    values = list(data.values())\n",
    "    lengths = [len(v.strip()) for v in values]# if len(v.strip()) > 50]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    min_length = min(lengths)\n",
    "    max_length = max(lengths)\n",
    "    median_length = median(lengths)\n",
    "    mean_length = mean(lengths)\n",
    "    \n",
    "    # Calculate mean number of newlines\n",
    "    newlines = [v.count('\\n') for v in (v.strip() for v in values)]\n",
    "    mean_newlines = mean(newlines)\n",
    "    \n",
    "    # Find most common first and last characters\n",
    "    first_chars = [v.strip()[0] for v in values if v]\n",
    "    last_chars = [v.strip()[-1] for v in values if v]\n",
    "    \n",
    "    first_char_counts = Counter(first_chars).most_common(5)\n",
    "    last_char_counts = Counter(last_chars).most_common(5)\n",
    "    \n",
    "    total_values = len(values)\n",
    "    first_char_counts = Counter(first_chars).most_common(5)\n",
    "    last_char_counts = Counter(last_chars).most_common(5)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"String Analysis Results:\")\n",
    "    print(f\"Minimum length: {min_length}\")\n",
    "    print(f\"Maximum length: {max_length}\")\n",
    "    print(f\"Median length: {median_length:.2f}\")\n",
    "    print(f\"Mean length: {mean_length:.2f}\")\n",
    "    print(f\"Mean newlines per value: {mean_newlines:.2f}\")\n",
    "    \n",
    "    print(\"\\nTop 5 first characters:\")\n",
    "    for char, count in first_char_counts:\n",
    "        percentage = (count / total_values) * 100\n",
    "        print(f\"  '{char}': {percentage:.1f}%\")\n",
    "    \n",
    "    print(\"\\nTop 5 last characters:\")\n",
    "    for char, count in last_char_counts:\n",
    "        percentage = (count / total_values) * 100\n",
    "        print(f\"  '{char}': {percentage:.1f}%\")\n",
    "\n",
    "file_path = '/Users/christopherackerman/repos/spar_self_recognition/summaries/xsum_train_llama3_8bchat_filteredlen_responses.json'\n",
    "analyze_json_strings(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis for source: FORUM\n",
      "Total count: 202\n",
      "Minimum length: 1\n",
      "Maximum length: 7346\n",
      "Median length: 1968.50\n",
      "Mean length: 2116.76\n",
      "Mean newlines per text: 12.32\n",
      "\n",
      "Top 5 first characters:\n",
      "  '.': 12.4% (25)\n",
      "  'I': 9.4% (19)\n",
      "  'A': 8.9% (18)\n",
      "  't': 6.4% (13)\n",
      "  'a': 6.4% (13)\n",
      "\n",
      "Top 5 last characters:\n",
      "  '.': 86.6% (175)\n",
      "  '!': 6.9% (14)\n",
      "  '?': 4.0% (8)\n",
      "  'a': 1.0% (2)\n",
      "  's': 0.5% (1)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Analysis for source: ABSTRACTS\n",
      "Total count: 350\n",
      "Minimum length: 87\n",
      "Maximum length: 4746\n",
      "Median length: 1898.00\n",
      "Mean length: 1900.70\n",
      "Mean newlines per text: 6.59\n",
      "\n",
      "Top 5 first characters:\n",
      "  '.': 17.1% (60)\n",
      "  't': 12.3% (43)\n",
      "  'o': 8.3% (29)\n",
      "  'a': 7.7% (27)\n",
      "  ',': 6.0% (21)\n",
      "\n",
      "Top 5 last characters:\n",
      "  '.': 100.0% (350)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Analysis for source: WIKIPEDIA\n",
      "Total count: 93\n",
      "Minimum length: 293\n",
      "Maximum length: 3105\n",
      "Median length: 1362.00\n",
      "Mean length: 1443.76\n",
      "Mean newlines per text: 5.04\n",
      "\n",
      "Top 5 first characters:\n",
      "  't': 15.1% (14)\n",
      "  ',': 9.7% (9)\n",
      "  'a': 9.7% (9)\n",
      "  'w': 7.5% (7)\n",
      "  '.': 7.5% (7)\n",
      "\n",
      "Top 5 last characters:\n",
      "  '.': 100.0% (93)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Analysis for source: DAILY_DIALOG\n",
      "Total count: 59\n",
      "Minimum length: 706\n",
      "Maximum length: 2843\n",
      "Median length: 1407.00\n",
      "Mean length: 1386.63\n",
      "Mean newlines per text: 8.17\n",
      "\n",
      "Top 5 first characters:\n",
      "  '.': 18.6% (11)\n",
      "  't': 8.5% (5)\n",
      "  ',': 6.8% (4)\n",
      "  'a': 6.8% (4)\n",
      "  'c': 5.1% (3)\n",
      "\n",
      "Top 5 last characters:\n",
      "  '.': 81.4% (48)\n",
      "  '!': 13.6% (8)\n",
      "  '?': 3.4% (2)\n",
      "  '\"': 1.7% (1)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Analysis for source: EU_AI\n",
      "Total count: 104\n",
      "Minimum length: 439\n",
      "Maximum length: 3421\n",
      "Median length: 1633.50\n",
      "Mean length: 1650.99\n",
      "Mean newlines per text: 5.71\n",
      "\n",
      "Top 5 first characters:\n",
      "  '.': 16.3% (17)\n",
      "  't': 11.5% (12)\n",
      "  'a': 9.6% (10)\n",
      "  'o': 8.7% (9)\n",
      "  's': 6.7% (7)\n",
      "\n",
      "Top 5 last characters:\n",
      "  '.': 100.0% (104)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Analysis for source: REDDIT\n",
      "Total count: 192\n",
      "Minimum length: 108\n",
      "Maximum length: 8165\n",
      "Median length: 1948.50\n",
      "Mean length: 2086.07\n",
      "Mean newlines per text: 12.07\n",
      "\n",
      "Top 5 first characters:\n",
      "  '.': 13.5% (26)\n",
      "  'I': 9.4% (18)\n",
      "  'a': 7.3% (14)\n",
      "  'A': 6.2% (12)\n",
      "  't': 6.2% (12)\n",
      "\n",
      "Top 5 last characters:\n",
      "  '.': 82.3% (158)\n",
      "  '!': 10.9% (21)\n",
      "  '?': 2.6% (5)\n",
      "  '\"': 1.0% (2)\n",
      "  'n': 0.5% (1)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Analysis for source: TOTAL\n",
      "Total count: 1000\n",
      "Minimum length: 1\n",
      "Maximum length: 8165\n",
      "Median length: 1798.50\n",
      "Mean length: 1881.14\n",
      "Mean newlines per text: 8.66\n",
      "\n",
      "Top 5 first characters:\n",
      "  '.': 14.6% (146)\n",
      "  't': 9.9% (99)\n",
      "  'a': 7.7% (77)\n",
      "  ',': 5.7% (57)\n",
      "  'o': 5.6% (56)\n",
      "\n",
      "Top 5 last characters:\n",
      "  '.': 92.8% (928)\n",
      "  '!': 4.3% (43)\n",
      "  '?': 1.5% (15)\n",
      "  '\"': 0.3% (3)\n",
      "  'a': 0.3% (3)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from statistics import mean, median\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def analyze_texts(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Group data by source\n",
    "    sources = defaultdict(list)\n",
    "    for item in data:\n",
    "        sources[item['source']].append(item['text'])\n",
    "    \n",
    "    # Function to analyze a list of texts\n",
    "    def analyze_texts_list(texts):\n",
    "        lengths = [len(t.strip()) for t in texts]\n",
    "        newlines = [t.count('\\n') for t in (t.strip() for t in texts)]\n",
    "        first_chars = [t.strip()[0] for t in texts if t]\n",
    "        last_chars = [t.strip()[-1] for t in texts if t]\n",
    "        \n",
    "        return {\n",
    "            'count': len(texts),\n",
    "            'min_length': min(lengths),\n",
    "            'max_length': max(lengths),\n",
    "            'median_length': median(lengths),\n",
    "            'mean_length': mean(lengths),\n",
    "            'mean_newlines': mean(newlines),\n",
    "            'first_chars': Counter(first_chars).most_common(5),\n",
    "            'last_chars': Counter(last_chars).most_common(5)\n",
    "        }\n",
    "    \n",
    "    # Analyze each source and total\n",
    "    results = {source: analyze_texts_list(texts) for source, texts in sources.items()}\n",
    "    results['total'] = analyze_texts_list([item['text'] for item in data])\n",
    "    \n",
    "    # Print results\n",
    "    for source, stats in results.items():\n",
    "        print(f\"\\nAnalysis for source: {source.upper()}\")\n",
    "        print(f\"Total count: {stats['count']}\")\n",
    "        print(f\"Minimum length: {stats['min_length']}\")\n",
    "        print(f\"Maximum length: {stats['max_length']}\")\n",
    "        print(f\"Median length: {stats['median_length']:.2f}\")\n",
    "        print(f\"Mean length: {stats['mean_length']:.2f}\")\n",
    "        print(f\"Mean newlines per text: {stats['mean_newlines']:.2f}\")\n",
    "        \n",
    "        print(\"\\nTop 5 first characters:\")\n",
    "        for char, count in stats['first_chars']:\n",
    "            percentage = (count / stats['count']) * 100\n",
    "            print(f\"  '{char}': {percentage:.1f}% ({count})\")\n",
    "        \n",
    "        print(\"\\nTop 5 last characters:\")\n",
    "        for char, count in stats['last_chars']:\n",
    "            percentage = (count / stats['count']) * 100\n",
    "            print(f\"  '{char}': {percentage:.1f}% ({count})\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Example usage\n",
    "file_path = '/Users/christopherackerman/repos/spar_self_recognition/completions_full/completions_llama3_8bchat_train.json'\n",
    "analyze_texts(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
